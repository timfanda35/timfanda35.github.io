<!doctype html><html lang=en-us>
<head>
<link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png>
<link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png>
<link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png>
<link rel=manifest href=/site.webmanifest>
<link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5>
<meta name=msapplication-TileColor content="#da532c">
<meta name=theme-color content="#ffffff">
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta http-equiv=x-ua-compatible content="ie=edge,chrome=1">
<title>機器學習筆記(week1) &#183; Bear Su's Blog - Running Bear!!! </title>
<meta name=generator content="Hugo 0.92.0">
<meta name=description content="Running Bear!!!">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://blog.bear-su.dev/images/bear.jpeg">
<meta name=twitter:title content="機器學習筆記(week1)">
<meta name=twitter:description content="最近朋友貼了這個課程給我:机器学习 - 斯坦福大学
總共有 11 週的課程
上班前看一部分，下班後再複習做做筆記
因為裡面的理論會用到微積分，只好在休息時間問問對數學比較拿手的朋友裡面的公式是如何推導的
有一種再補大學學債的感覺
第一週的主題為：

Introduction
Linear Regression with One Variable
Linear Algebra Review
">
<meta property="og:title" content="機器學習筆記(week1)">
<meta property="og:description" content="最近朋友貼了這個課程給我:机器学习 - 斯坦福大学
總共有 11 週的課程
上班前看一部分，下班後再複習做做筆記
因為裡面的理論會用到微積分，只好在休息時間問問對數學比較拿手的朋友裡面的公式是如何推導的
有一種再補大學學債的感覺
第一週的主題為：

Introduction
Linear Regression with One Variable
Linear Algebra Review
">
<meta property="og:type" content="article">
<meta property="og:url" content="https://blog.bear-su.dev/2017/05/06/machine-learning-note-week1/"><meta property="og:image" content="https://blog.bear-su.dev/images/bear.jpeg"><meta property="article:section" content>
<meta property="article:published_time" content="2017-05-06T08:13:00+08:00">
<meta property="article:modified_time" content="2017-05-06T08:13:00+08:00">
<link href rel=alternate type=application/rss+xml title="Bear Su's Blog">
<link href rel=feed type=application/rss+xml title="Bear Su's Blog">
<meta itemprop=name content="機器學習筆記(week1)">
<meta itemprop=description content="最近朋友貼了這個課程給我:机器学习 - 斯坦福大学
總共有 11 週的課程
上班前看一部分，下班後再複習做做筆記
因為裡面的理論會用到微積分，只好在休息時間問問對數學比較拿手的朋友裡面的公式是如何推導的
有一種再補大學學債的感覺
第一週的主題為：

Introduction
Linear Regression with One Variable
Linear Algebra Review
"><meta itemprop=datePublished content="2017-05-06T08:13:00+08:00">
<meta itemprop=dateModified content="2017-05-06T08:13:00+08:00">
<meta itemprop=wordCount content="503"><meta itemprop=image content="https://blog.bear-su.dev/images/bear.jpeg">
<meta itemprop=keywords content>
<link href="https://fonts.googleapis.com/css?family=Lato:400" rel=stylesheet>
<link rel=stylesheet href=https://blog.bear-su.dev/css/style.css>
<script type=application/ld+json>{"@context":"http://schema.org","@type":"WebSite","name":"Bear Su\u0027s Blog","url":"https:\/\/blog.bear-su.dev\/2017\/05\/06\/machine-learning-note-week1\/"}</script>
</head>
<body>
<header class=gheader>
<div class=container>
<nav class=navbar>
<div class=navbar-brand>
<a class=navbar-item href=https://blog.bear-su.dev>
<p class=brand>Bear Su's Blog</p>
</a>
<div class="navbar-burger burger" data-target=navMenubd-example>
<span></span>
<span></span>
<span></span>
</div>
</div>
<div id=navMenubd-example class=navbar-menu>
<div class=navbar-start>
<a class=navbar-item href=/categories/>
Categories
</a>
<a class=navbar-item href=https://github.com/timfanda35>
GitHub
</a>
<a class=navbar-item href=https://www.linkedin.com/in/bearsu>
Linkedin
</a>
<a class=navbar-item href=https://twitter.com/Fandayo>
Twitter
</a>
</div>
</div>
</nav>
</div>
</header>
<div class=main-content>
<div class=container>
<article class=post>
<header class=post-header>
<h1 class=title>機器學習筆記(week1)</h1>
<div class=meta>
<time class=pub-time datetime=2017-05-06T08:13:00+08:00>
2017.05.06
</time>
<div class=terms>
</div>
</div>
</header>
<div class=post-body>
<div class=toc-wrap>
<h2>Table of Contents</h2>
<nav id=TableOfContents>
<ul>
<li><a href=#introduction>Introduction</a>
<ul>
<li><a href=#機器學習的定義>機器學習的定義</a></li>
<li><a href=#演算法>演算法</a></li>
</ul>
</li>
<li><a href=#linear-regression-with-one-variable>Linear Regression with One Variable</a></li>
<li><a href=#linear-algebra-review>Linear Algebra Review</a></li>
<li><a href=#參考>參考</a></li>
</ul>
</nav>
</div>
<p>最近朋友貼了這個課程給我:<a href=https://www.coursera.org/learn/machine-learning/home/welcome>机器学习 - 斯坦福大学</a></p>
<p>總共有 11 週的課程</p>
<p>上班前看一部分，下班後再複習做做筆記</p>
<p>因為裡面的理論會用到微積分，只好在休息時間問問對數學比較拿手的朋友裡面的公式是如何推導的</p>
<p>有一種再補大學學債的感覺</p>
<p>第一週的主題為：</p>
<ol>
<li>Introduction</li>
<li>Linear Regression with One Variable</li>
<li>Linear Algebra Review</li>
</ol>
<h2 id=introduction>Introduction <a href=#introduction>¶</a></h2>
<hr>
<h3 id=機器學習的定義>機器學習的定義 <a href=#%e6%a9%9f%e5%99%a8%e5%ad%b8%e7%bf%92%e7%9a%84%e5%ae%9a%e7%be%a9>¶</a></h3>
<blockquote>
<p>Tom Mitchell (1998) Well-posed Learning Problem: A computer program is said to <em>learn</em> from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.</p>
</blockquote>
<p>T: task
P: performance
E: experience</p>
<p>以讓程式學習如何過濾垃圾信件為例：</p>
<p>T: 將信件分類正常信件與垃圾信件
P: 判斷的正確率
E: 觀察你將信件標記為正常信件或垃圾信件</p>
<h3 id=演算法>演算法 <a href=#%e6%bc%94%e7%ae%97%e6%b3%95>¶</a></h3>
<p>先提到有：</p>
<ul>
<li>監督式學習 (Supervised learning)
<ul>
<li>先前可以得知訓練資料中哪些為正確答案</li>
<li>分類問題(Classification Problem): Discrete valued output(0 or 1)</li>
<li>回歸問題(Regression Problem): Goal is predict a continuous valued output</li>
</ul>
</li>
<li>非監督式學習 (Unsupervised learning)
<ul>
<li>先前無法得知訓練資料中哪些為正確答案</li>
</ul>
</li>
</ul>
<h2 id=linear-regression-with-one-variable>Linear Regression with One Variable <a href=#linear-regression-with-one-variable>¶</a></h2>
<hr>
<p>線性回歸函數：</p>
<pre tabindex=0><code class=language-mathjax data-lang=mathjax>h_{\theta_0}(x) = \theta_0 + \theta_1x \\
</code></pre><p>要如何用大量訓練資料找出適合的</p>
<pre tabindex=0><code class=language-mathjax data-lang=mathjax>\theta_0, \theta_1 \\
</code></pre><p>使函數最接近訓練資料的分佈，進而提升函數預測的精準度</p>
<p>以下是 <code>cost function</code>，值越小，函數也就越接近訓練資料的分佈</p>
<pre tabindex=0><code class=language-mathjax data-lang=mathjax>J(\theta_0, \theta_1) = \frac1{2m}\sum_{i=1}^m(h_{\theta_0}(x^{(i)}) - y^{(i)})^2 \\
</code></pre><p>可以使用梯度下降法(Gradient Descent)來最小化，不過無法保證得到最佳解</p>
<pre tabindex=0><code class=language-mathjax data-lang=mathjax>J(\theta_0, \theta_1) \\
</code></pre><p>梯度下降法(Gradient Descent)的公式為：</p>
<pre tabindex=0><code class=language-mathjax data-lang=mathjax>	\theta_j := \theta_j - \alpha {\frac\partial{\partial\theta_i}} J(\theta_0, \theta_1) \\
  \text (for\ j = 0\ and\ j = 1) \\
  \text (\alpha\ is\ learning\ rate\ 會影響收斂的速度) \\
</code></pre><p>將 <code>cost funciton</code> 代入：</p>
<pre tabindex=0><code class=language-mathjax data-lang=mathjax>\newcommand{\hf}{h_{\theta_0}(x^{(i)}) - y^{(i)}}
\newcommand{\ptz}{\frac\partial{\partial\theta_0}}
\begin{equation}
    \eqalign{
		\ptz J(\theta_0, \theta_1) &amp; = \ptz \frac1{2m}\sum_{i=1}^m(\hf)^2 \\
		&amp; = 2 \times \frac1{2m}\sum_{i=1}^m(\hf) \times \ptz (\hf) \\
		&amp; = \frac1{m}\sum_{i=1}^m(\hf) \times \ptz (\hf) \\
		&amp; = \frac1{m}\sum_{i=1}^m(\hf) \times \ptz (\theta_0 + \theta_1x^{(i)} - y^{(i)}) \\
		&amp; = \frac1{m}\sum_{i=1}^m(\hf) \times 1 \\
		&amp; = \frac1{m}\sum_{i=1}^m(\hf)
    }
\end{equation}
\\
</code></pre><pre tabindex=0><code class=language-mathjax data-lang=mathjax>\newcommand{\hf}{h_{\theta_0}(x^{(i)}) - y^{(i)}}
\newcommand{\pto}{\frac\partial{\partial\theta_1}}
\begin{equation}
    \eqalign{
		\pto J(\theta_0, \theta_1) &amp; = \pto \frac1{2m}\sum_{i=1}^m(\hf)^2 \\
		&amp; = 2 \times \frac1{2m}\sum_{i=1}^m(\hf) \times \pto (\hf) \\
		&amp; = \frac1{m}\sum_{i=1}^m(\hf) \times \pto (\hf) \\
		&amp; = \frac1{m}\sum_{i=1}^m(\hf) \times \pto (\theta_0 + \theta_1x^{(i)} - y^{(i)}) \\
		&amp; = \frac1{m}\sum_{i=1}^m(\hf) \times x^{(i)}
    }
\end{equation}
\\
</code></pre><p>最後的演算法：</p>
<pre tabindex=0><code class=language-mathjax data-lang=mathjax>\newcommand{\hf}{h_{\theta_0}(x^{(i)}) - y^{(i)}}
\newcommand{\pto}{\frac\partial{\partial\theta_1}}
repest\ until\ convergence\ \{ \\
\begin{equation}
    \eqalign{
    	\theta_0 &amp;:= \theta_0 - \alpha \frac1{m}\sum_{i=1}^m(\hf) \\
      \theta_1 &amp;:= \theta_1 - \alpha \frac1{m}\sum_{i=1}^m(\hf) \times x^{(i)}
    }
\end{equation} \\
\}
</code></pre><h2 id=linear-algebra-review>Linear Algebra Review <a href=#linear-algebra-review>¶</a></h2>
<hr>
<p>介紹矩陣 Matrix 的基本運算</p>
<p><code>row</code> 的值是由上往下遞增</p>
<p><code>column</code> 的值是由左往右遞增</p>
<p>本門課 Matrix index 的值是由 <code>1</code> 開始遞增，而不是如程式語言由 <code>0</code> 開始遞增</p>
<p>標記方法為:</p>
<pre tabindex=0><code class=language-mathjax data-lang=mathjax>\eqalign{
&amp;R^{m \times n} \\
&amp;m:\ rows \\
&amp;n:\ columns \\ 
} \\
</code></pre><p>Vector 是 <code>m x 1</code> 的 Matrix</p>
<pre tabindex=0><code class=language-mathjax data-lang=mathjax>\eqalign{
&amp;R^{m} \\
&amp;m:\ rows \\
} \\
</code></pre><p>取值的時候：</p>
<pre tabindex=0><code class=language-mathjax data-lang=mathjax>\eqalign{
&amp;A_{ij} \\
&amp;i:\ row \\
&amp;j:\ column \\
} \\
</code></pre><p>Iedentity Matrix(用 <code>I</code> 表示):</p>
<p>一定是</p>
<pre tabindex=0><code class=language-mathjax data-lang=mathjax>\eqalign{
&amp; I\ 或\ I^{m \times m} \\
&amp; 而且對於任何\ Matrix\ A:\\
&amp; A \times I = I \times A = A
} \\
</code></pre><p>例如：</p>
<pre tabindex=0><code class=language-mathjax data-lang=mathjax>\begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 \\
\end{bmatrix}
\\
</code></pre><p>Matrix inverse(If A is an <code>m x m</code> matrix, and if it has an inverse):</p>
<pre tabindex=0><code class=language-mathjax data-lang=mathjax>AA^{-1} = A^{-1}A = I
\\
</code></pre><p>Matrix Trunspose:</p>
<pre tabindex=0><code class=language-mathjax data-lang=mathjax>A =
\begin{bmatrix}
1 &amp; 2 &amp; 0 \\
3 &amp; 5 &amp; 9 \\
\end{bmatrix}
\\
B =
A^T =
\begin{bmatrix}
1 &amp; 2 &amp; 0 \\
3 &amp; 5 &amp; 9 \\
\end{bmatrix}
\\
B_{ij} = A_{ji} \\
</code></pre><h2 id=參考>參考 <a href=#%e5%8f%83%e8%80%83>¶</a></h2>
<hr>
<p><a href=https://medium.com/@ken90242/machine-learning%E5%AD%B8%E7%BF%92%E6%97%A5%E8%A8%98-coursera%E7%AF%87-introduction-supervised-learning-unsupervised-learning-18000f2b9ca1>Machine Learning學習日記 — Coursera篇 (Week 1.1):Supervised learning,Unsupervised learning,regression,classification</a></p>
<p><a href=http://murphymind.blogspot.tw/2017/03/linear.regression.html>[學習筆記] 機器學習: 單變數線性回歸 (Machine learning: Linear Regression with One Variable)</a></p>
<p><a href=http://ocw.aca.ntu.edu.tw/ntu-ocw/ocw/cou/100S111/42>臺大開放式課程 微積分 第12章 - 偏導數</a></p>
<p><a href=http://blog.ponan.li/post/2013/08/03/write-complex-latex-equations-in-logdown-by-mathjax-support>在 logdown 裡面打複雜方程式</a></p>
</div>
<footer class=post-footer>
<hr>
<div style=margin-top:2.5rem>
<div style=display:flex;justify-content:center;width:100%>
<p style=color:gray>
如果覺得這篇文章對您有所幫助，歡迎贊助我一杯咖啡 ☕️
</p>
</div>
<div style=display:flex;justify-content:center;width:100%>
<p style=color:gray>
祝您有美好的一天 ❤️
</p>
</div>
<div style=display:flex;justify-content:center;width:100%;margin-top:1rem>
<script type=text/javascript src=https://cdnjs.buymeacoffee.com/1.0.0/button.prod.min.js data-name=bmc-button data-slug=timfanda35 data-color=#FFDD00 data-emoji data-font=Bree data-text="Buy me a coffee" data-outline-color=#000000 data-font-color=#000000 data-coffee-color=#ffffff></script>
</div>
</div>
<hr>
<div style=margin-top:1rem>
<script src=https://utteranc.es/client.js repo=timfanda35/timfanda35.github.io issue-term=pathname label=comment theme=github-light crossorigin=anonymous async></script>
</div>
<div class=share>
</div>
<div class=pagenation>
<ul class=pagenation-list>
<li class="pager previous">
<a href=https://blog.bear-su.dev/2017/04/19/google-cloud-onboard-20170419/ data-toggle=tooltip data-placement=top title="Google Cloud OnBoard 20170419">&larr; Previous Post</a>
</li>
<li class="pager next">
<a href=https://blog.bear-su.dev/2017/05/17/ruby-by-using-treetype-to-draw-text-to-a-ascii-art/ data-toggle=tooltip data-placement=top title="Ruby 透過 FreeType 產生點陣文字">Next Post &rarr;</a>
</li>
</ul>
</div>
</footer>
</article>
<article class=related></article>
<script type=application/ld+json>{"@context":"http://schema.org","@type":"NewsArticle","mainEntityOfPage":{"@type":"NewsArticle","@id":"https:\/\/blog.bear-su.dev\/2017\/05\/06\/machine-learning-note-week1\/","headline":"最近朋友貼了這個課程給我:机器学习 - 斯坦福大学總共有 11 週的課程上班前看一部分，下班後再複習做做筆記因為裡面的理論會用到微積分，只好在休息時間問問對數學比較拿手的朋友裡面的公式是如何推導的有一種再補大學學債的感覺","author":"Bear Su","publisher":{"@type":"Organization","name":"Bear Su","logo":""},"image":"","datePublished":"2017-05-06"},"headline":"最近朋友貼了這個課程給我:机器学习 - 斯坦福大学總共有 11 週的課程上班前看一部分，下班後再複習做做筆記因為裡面的理論會用到微積分，只好在休息時間問問對數學比較拿手的朋友裡面的公式是如何推導的有一種再補大學學債的感覺","alternativeHeadline":"Bear Su\u0027s Blog","datePublished":"2017-05-06","dateModified":"2017-05-06","url":"https:\/\/blog.bear-su.dev\/2017\/05\/06\/machine-learning-note-week1\/","wordCount":"503","author":{"@type":"Person","name":"Bear Su"},"publisher":{"@type":"Organization","name":"Bear Su"},"image":"","description":""}</script>
<script type=application/ld+json>{"@context":"http://schema.org","@type":"Person","@id":"https:\/\/blog.bear-su.dev\/2017\/05\/06\/machine-learning-note-week1\/","name":"Bear Su"}</script>
</div>
</div>
<footer class=gfooter>
<div class=inner>
<div class=container>
<p class=copy>
<small>Copyright © 2015 - 2022 Bear Su's Blog</small>
<small> | 本部落格有使用 Google Analytic 及 Cookie</small>
</p>
<nav class=footer-nav>
<div class=inner>
</div>
</nav>
</div>
</div>
</footer>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-KJXNT5CQCS"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-KJXNT5CQCS',{anonymize_ip:!1})}</script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js integrity="sha512-894YE6QWD5I59HgZOGReFYm4dnWc1Qt5NtvYSaNcOP+u1T9qYdvdihz0PPSiiqn/+/3e7Jo4EaG7TubfWGUrMQ==" crossorigin=anonymous referrerpolicy=no-referrer></script>
<script src=https://blog.bear-su.dev/js/script.js></script>
</body>
</html>