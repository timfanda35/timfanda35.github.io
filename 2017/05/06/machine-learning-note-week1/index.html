<!DOCTYPE html>
<html lang="en-us">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge,chrome=1">
  <title>機器學習筆記(week1) &middot; Bear Su&#39;s Blog -  Running Bear!!! </title>
  <meta name="generator" content="Hugo 0.53" />
  <meta name="description" content="Running Bear!!!"> 

  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="機器學習筆記(week1)"/>
<meta name="twitter:description" content="最近朋友貼了這個課程給我:机器学习 - 斯坦福大学

總共有 11 週的課程

上班前看一部分，下班後再複習做做筆記

因為裡面的理論會用到微積分，只好在休息時間問問對數學比較拿手的朋友裡面的公式是如何推導的

有一種再補大學學債的感覺

第一週的主題為：


Introduction
Linear Regression with One Variable
Linear Algebra Review
"/>

  <meta property="og:title" content="機器學習筆記(week1)" />
<meta property="og:description" content="最近朋友貼了這個課程給我:机器学习 - 斯坦福大学

總共有 11 週的課程

上班前看一部分，下班後再複習做做筆記

因為裡面的理論會用到微積分，只好在休息時間問問對數學比較拿手的朋友裡面的公式是如何推導的

有一種再補大學學債的感覺

第一週的主題為：


Introduction
Linear Regression with One Variable
Linear Algebra Review
" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://timfanda35.github.io/2017/05/06/machine-learning-note-week1/" /><meta property="article:published_time" content="2017-05-06T08:13:00&#43;08:00"/>
<meta property="article:modified_time" content="2017-05-06T08:13:00&#43;08:00"/>


  <!-- RSS autodiscovery --> 

  
<meta itemprop="name" content="機器學習筆記(week1)">
<meta itemprop="description" content="最近朋友貼了這個課程給我:机器学习 - 斯坦福大学

總共有 11 週的課程

上班前看一部分，下班後再複習做做筆記

因為裡面的理論會用到微積分，只好在休息時間問問對數學比較拿手的朋友裡面的公式是如何推導的

有一種再補大學學債的感覺

第一週的主題為：


Introduction
Linear Regression with One Variable
Linear Algebra Review
">


<meta itemprop="datePublished" content="2017-05-06T08:13:00&#43;08:00" />
<meta itemprop="dateModified" content="2017-05-06T08:13:00&#43;08:00" />
<meta itemprop="wordCount" content="503">



<meta itemprop="keywords" content="" />

  <link href="https://fonts.googleapis.com/css?family=Lato:400" rel="stylesheet">
  <link rel="stylesheet" href="https://timfanda35.github.io/css/style.css">
  <script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "WebSite",
  "name": "Bear Su&#39;s Blog",
  "url": "https://timfanda35.github.io/2017/05/06/machine-learning-note-week1/"
}
</script>

</head>

<body>
  <header class="gheader">
  <div class="container">
    <nav class="navbar ">
      <div class="navbar-brand">
        <a class="navbar-item" href="https://timfanda35.github.io">
          <h1 class="brand">Bear Su&#39;s Blog</h1>
        </a>

        <div class="navbar-burger burger" data-target="navMenubd-example">
          <span></span>
          <span></span>
          <span></span>
        </div>
      </div>

      <div id="navMenubd-example" class="navbar-menu">
        <div class="navbar-start">
          
          <a class="navbar-item" href="/categories/">
            Categories
          </a>
          
          <a class="navbar-item" href="https://github.com/timfanda35">
            GitHub
          </a>
          
          <a class="navbar-item" href="https://www.linkedin.com/in/bearsu">
            Linkedin
          </a>
          
          <a class="navbar-item" href="https://twitter.com/Fandayo">
            Twitter
          </a>
          
        </div>
      </div>
    </nav>
  </div>
</header>


  <div class="main-content">
    <div class="container">
      
  <article class="post">
    <header class="post-header">
      <h1 class="title">機器學習筆記(week1)</h1>
      <div class="meta">
        <time class="pub-time"  datetime="2017-05-06T08:13:00&#43;08:00">
          2017.05.06
        </time>
        <div class="terms">
          
          
        </div>
      </div>
    </header>
    <div class="post-body">
      <p>最近朋友貼了這個課程給我:<a href="https://www.coursera.org/learn/machine-learning/home/welcome">机器学习 - 斯坦福大学</a></p>

<p>總共有 11 週的課程</p>

<p>上班前看一部分，下班後再複習做做筆記</p>

<p>因為裡面的理論會用到微積分，只好在休息時間問問對數學比較拿手的朋友裡面的公式是如何推導的</p>

<p>有一種再補大學學債的感覺</p>

<p>第一週的主題為：</p>

<ol>
<li>Introduction</li>
<li>Linear Regression with One Variable</li>
<li>Linear Algebra Review</li>
</ol>

<h2 id="introduction">Introduction</h2>

<h3 id="機器學習的定義">機器學習的定義</h3>

<blockquote>
<p>Tom Mitchell (1998) Well-posed Learning Problem: A computer program is said to <em>learn</em> from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.</p>
</blockquote>

<p>T: task
P: performance
E: experience</p>

<p>以讓程式學習如何過濾垃圾信件為例：</p>

<p>T: 將信件分類正常信件與垃圾信件
P: 判斷的正確率
E: 觀察你將信件標記為正常信件或垃圾信件</p>

<h3 id="演算法">演算法</h3>

<p>先提到有：
* 監督式學習 (Supervised learning)
    * 先前可以得知訓練資料中哪些為正確答案
    * 分類問題(Classification Problem): Discrete valued output(0 or 1)
  * 回歸問題(Regression Problem): Goal is predict a continuous valued output
* 非監督式學習 (Unsupervised learning)
    * 先前無法得知訓練資料中哪些為正確答案</p>

<h2 id="linear-regression-with-one-variable">Linear Regression with One Variable</h2>

<p>線性回歸函數：</p>

<pre><code class="language-mathjax">h_{\theta_0}(x) = \theta_0 + \theta_1x \\
</code></pre>

<p>要如何用大量訓練資料找出適合的</p>

<pre><code class="language-mathjax">\theta_0, \theta_1 \\
</code></pre>

<p>使函數最接近訓練資料的分佈，進而提升函數預測的精準度</p>

<p>以下是 <code>cost function</code>，值越小，函數也就越接近訓練資料的分佈</p>

<pre><code class="language-mathjax">J(\theta_0, \theta_1) = \frac1{2m}\sum_{i=1}^m(h_{\theta_0}(x^{(i)}) - y^{(i)})^2 \\
</code></pre>

<p>可以使用梯度下降法(Gradient Descent)來最小化，不過無法保證得到最佳解</p>

<pre><code class="language-mathjax">J(\theta_0, \theta_1) \\
</code></pre>

<p>梯度下降法(Gradient Descent)的公式為：</p>

<pre><code class="language-mathjax">	\theta_j := \theta_j - \alpha {\frac\partial{\partial\theta_i}} J(\theta_0, \theta_1) \\
  \text (for\ j = 0\ and\ j = 1) \\
  \text (\alpha\ is\ learning\ rate\ 會影響收斂的速度) \\
</code></pre>

<p>將 <code>cost funciton</code> 代入：</p>

<pre><code class="language-mathjax">\newcommand{\hf}{h_{\theta_0}(x^{(i)}) - y^{(i)}}
\newcommand{\ptz}{\frac\partial{\partial\theta_0}}
\begin{equation}
    \eqalign{
		\ptz J(\theta_0, \theta_1) &amp; = \ptz \frac1{2m}\sum_{i=1}^m(\hf)^2 \\
		&amp; = 2 \times \frac1{2m}\sum_{i=1}^m(\hf) \times \ptz (\hf) \\
		&amp; = \frac1{m}\sum_{i=1}^m(\hf) \times \ptz (\hf) \\
		&amp; = \frac1{m}\sum_{i=1}^m(\hf) \times \ptz (\theta_0 + \theta_1x^{(i)} - y^{(i)}) \\
		&amp; = \frac1{m}\sum_{i=1}^m(\hf) \times 1 \\
		&amp; = \frac1{m}\sum_{i=1}^m(\hf)
    }
\end{equation}
\\
</code></pre>

<pre><code class="language-mathjax">\newcommand{\hf}{h_{\theta_0}(x^{(i)}) - y^{(i)}}
\newcommand{\pto}{\frac\partial{\partial\theta_1}}
\begin{equation}
    \eqalign{
		\pto J(\theta_0, \theta_1) &amp; = \pto \frac1{2m}\sum_{i=1}^m(\hf)^2 \\
		&amp; = 2 \times \frac1{2m}\sum_{i=1}^m(\hf) \times \pto (\hf) \\
		&amp; = \frac1{m}\sum_{i=1}^m(\hf) \times \pto (\hf) \\
		&amp; = \frac1{m}\sum_{i=1}^m(\hf) \times \pto (\theta_0 + \theta_1x^{(i)} - y^{(i)}) \\
		&amp; = \frac1{m}\sum_{i=1}^m(\hf) \times x^{(i)}
    }
\end{equation}
\\
</code></pre>

<p>最後的演算法：</p>

<pre><code class="language-mathjax">\newcommand{\hf}{h_{\theta_0}(x^{(i)}) - y^{(i)}}
\newcommand{\pto}{\frac\partial{\partial\theta_1}}
repest\ until\ convergence\ \{ \\
\begin{equation}
    \eqalign{
    	\theta_0 &amp;:= \theta_0 - \alpha \frac1{m}\sum_{i=1}^m(\hf) \\
      \theta_1 &amp;:= \theta_1 - \alpha \frac1{m}\sum_{i=1}^m(\hf) \times x^{(i)}
    }
\end{equation} \\
\}
</code></pre>

<h2 id="linear-algebra-review">Linear Algebra Review</h2>

<p>介紹矩陣 Matrix 的基本運算</p>

<p><code>row</code> 的值是由上往下遞增</p>

<p><code>column</code> 的值是由左往右遞增</p>

<p>本門課 Matrix index 的值是由 <code>1</code> 開始遞增，而不是如程式語言由 <code>0</code> 開始遞增</p>

<p>標記方法為:</p>

<pre><code class="language-mathjax">\eqalign{
&amp;R^{m \times n} \\
&amp;m:\ rows \\
&amp;n:\ columns \\ 
} \\
</code></pre>

<p>Vector 是 <code>m x 1</code> 的 Matrix</p>

<pre><code class="language-mathjax">\eqalign{
&amp;R^{m} \\
&amp;m:\ rows \\
} \\
</code></pre>

<p>取值的時候：</p>

<pre><code class="language-mathjax">\eqalign{
&amp;A_{ij} \\
&amp;i:\ row \\
&amp;j:\ column \\
} \\
</code></pre>

<p>Iedentity Matrix(用 <code>I</code> 表示):</p>

<p>一定是</p>

<pre><code class="language-mathjax">\eqalign{
&amp; I\ 或\ I^{m \times m} \\
&amp; 而且對於任何\ Matrix\ A:\\
&amp; A \times I = I \times A = A
} \\
</code></pre>

<p>例如：</p>

<pre><code class="language-mathjax">\begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 \\
\end{bmatrix}
\\
</code></pre>

<p>Matrix inverse(If A is an <code>m x m</code> matrix, and if it has an inverse):</p>

<pre><code class="language-mathjax">AA^{-1} = A^{-1}A = I
\\
</code></pre>

<p>Matrix Trunspose:</p>

<pre><code class="language-mathjax">A =
\begin{bmatrix}
1 &amp; 2 &amp; 0 \\
3 &amp; 5 &amp; 9 \\
\end{bmatrix}
\\
B =
A^T =
\begin{bmatrix}
1 &amp; 2 &amp; 0 \\
3 &amp; 5 &amp; 9 \\
\end{bmatrix}
\\
B_{ij} = A_{ji} \\
</code></pre>

<h2 id="參考">參考</h2>

<p><a href="https://medium.com/@ken90242/machine-learning%E5%AD%B8%E7%BF%92%E6%97%A5%E8%A8%98-coursera%E7%AF%87-introduction-supervised-learning-unsupervised-learning-18000f2b9ca1">Machine Learning學習日記 — Coursera篇 (Week 1.1):Supervised learning,Unsupervised learning,regression,classification</a></p>

<p><a href="http://murphymind.blogspot.tw/2017/03/linear.regression.html">[學習筆記] 機器學習: 單變數線性回歸 (Machine learning: Linear Regression with One Variable)</a></p>

<p><a href="http://ocw.aca.ntu.edu.tw/ntu-ocw/ocw/cou/100S111/42">臺大開放式課程 微積分 第12章 - 偏導數</a></p>

<p><a href="http://blog.ponan.li/post/2013/08/03/write-complex-latex-equations-in-logdown-by-mathjax-support">在 logdown 裡面打複雜方程式</a></p>
    </div>
    <footer class="post-footer">
      
      <div class="share">
        
      </div>

      <div class="pagenation">
        <ul class="pagenation-list">
          
          <li class="pager previous">
            <a href="https://timfanda35.github.io/2017/04/19/google-cloud-onboard-20170419/" data-toggle="tooltip" data-placement="top" title="Google Cloud OnBoard 20170419">&larr; Previous Post</a>
          </li>
          
          
          <li class="pager next">
            <a href="https://timfanda35.github.io/2017/05/17/ruby-by-using-treetype-to-draw-text-to-a-ascii-art/" data-toggle="tooltip" data-placement="top" title="Ruby 透過 FreeType 產生點陣文字">Next Post &rarr;</a>
          </li>
          
        </ul>
      </div>
    </footer>
  </article>

  
  <article class="related"></article>

  <script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "NewsArticle",
  "mainEntityOfPage": {
    "@type": "NewsArticle",
    "@id": "https://timfanda35.github.io/2017/05/06/machine-learning-note-week1/",
    "headline": "最近朋友貼了這個課程給我:机器学习 - 斯坦福大学總共有 11 週的課程上班前看一部分，下班後再複習做做筆記因為裡面的理論會用到微積分，只好在休息時間問問對數學比較拿手的朋友裡面的公式是如何推導的有一種再補大學學債的感覺",
    "author": "Bear Su",
    "publisher": {
      "@type": "Organization",
      "name": "Bear Su",
      "logo": ""
    },
    "image": "",
    "datePublished": "2017-05-06"
  },
  "headline": "最近朋友貼了這個課程給我:机器学习 - 斯坦福大学總共有 11 週的課程上班前看一部分，下班後再複習做做筆記因為裡面的理論會用到微積分，只好在休息時間問問對數學比較拿手的朋友裡面的公式是如何推導的有一種再補大學學債的感覺",
  "alternativeHeadline": "Bear Su&#39;s Blog",
  "datePublished": "2017-05-06",
  "dateModified": "2017-05-06",
  "url": "https://timfanda35.github.io/2017/05/06/machine-learning-note-week1/",
  "wordCount": "503",
  "author": {
    "@type": "Person",
    "name": "Bear Su"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Bear Su"
  },
  "image": "",
  
  
  "description": ""
}
</script>

  <script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Person",
  "@id": "https://timfanda35.github.io/2017/05/06/machine-learning-note-week1/",
  "name": "Bear Su"
}
</script>


    </div>
  </div>

  <footer class="gfooter">
  <div class="inner">
    <div class="container">
      <p class="copy"><small>(c) 2019 Bear Su&#39;s Blog</small></p>

      <nav class="footer-nav">
        <div class="inner">
          
        </div>
      </nav>
    </div>
  </div>
</footer>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-5708312-7', 'auto');
	
	ga('send', 'pageview');
}
</script>


<script
  src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
<script src="https://timfanda35.github.io/js/script.js"></script>

</body>
</html>
